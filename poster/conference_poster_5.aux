\relax 
\citation{bib1}
\citation{bib2}
\citation{bib3}
\citation{jordan2004introduction}
\citation{jordan2004introduction}
\citation{bib1}
\citation{bib1}
\citation{*}
\bibstyle{unsrtnat}
\bibdata{sample}
\bibcite{jordan2004introduction}{Jordan and Bishop(1997)}
\bibcite{bib1}{Palocsay et\nobreakspace  {}al.(2000)Palocsay, Wang, and Brookshire}
\bibcite{bib2}{Chung et\nobreakspace  {}al.(2003)Chung, Schmidt, and Witte}
\bibcite{bib3}{Schmidt and Witte(1988)}
\bibcite{bib4}{Murphy(2012)}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \color  {Green} Feedforward Neural Network\cite  {jordan2004introduction}\relax }}{1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \color  {Green} Validation set performance for NN predictor\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \color  {Green} 1978 Dataset\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \color  {Green} Number of trees and number of features for the top 5\% test scores. Simulations were performed for a feature number ranging from $1$ to all, a number of trees ranging from $10$ to $100$, and a Gini loss.\relax }}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \color  {Green} Best performance for the Random Forest predictor. Test set is identical for NN and Random Forest.\relax }}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \color  {Green} Importance weights for the top 2 test scores among all classifiers simulated.\relax }}{1}}
